\chapter{Theoretical Background} 

\label{theory} 


% should contain
% metagenomics/microbiome
% ML -> RF, XGB, SVM
% Workflowmanager for reproducibility -> Nextflow
% wich of these aproaches has been used before? -> metapheno, metaml

\section{Metagenomics}

Real world data is ugly. It contains a lot of noise. Microbiome reads can be contaminated with host DNA and with adapters from sequencing. In order to clean up the data preprocessing has to be performed. 

% single end, pair end where to put it exactly??


	\subsection{File formats}
Sequencing is the technique of translating the succession of nucleotides into a string. In order to save this digitized DNA information two standard file formats have prevailed: FASTA and FASTQ. 

FASTA is a relatively simple format. It uses two lines per sequence. A read begins with the character \code{>} followed by the name of the read (unique identifier), a newline character \code{\textbackslash n} and then the sequence of the read which is usually ended with a newline character, then this can be repeated in case of multiple reads. The DNA sequence usually is encoded with the letters A,C,T and G (can also be lower case) for the bases Adenine, Cytosine, Thymine and Guanine.
% maybe to detailed
The general file suffix is .fasta. The FASTA file format can also represent amino acid so in order to distinguish the two the recommended suffixes are .faa for amino acids and .fna for nucleotides.
FASTQ encodes the sequence as well as its corresponding quality score. Per sequence 4 lines are used. 
The first line begins with the \code{@} character followed by the sequence identifier (systematic use with Illumina, encodes machine used, lane, etc).
The second line contains the raw sequence.
The third line starts with the \code{+} character optionally followed by the same sequence identifier as in used the first line.
The fouth line contains the encoded quality scores corresponding to the second line, therefore it has to have the same number of symbols than the sequence itself.
% a rough explanation of it
The quality scores are encoded using the order of ASCII (American Standard of Code Information Interchange) characters. Starting with the 33rd ASCII character \code{!} representing lowest quality the original encoding goes up to character 126 \code{\textasciitilde} as the highest encoded quality score possible. Today the most widely used encoding is called Pred+33 (i.e. used by Illumina). It only encodes up to character 74 \code{J} as highest quality score.

	\subsection{Preprocessing}
The quality information in a FASTQ file can be leveraged to only keep part of a sequence which are reliable. If the quality score falls below a set threshold reads can be cut so that only meaningful data is used.

When using microbiome reads there is always the possibility of reads still containing host DNA. In order to clean up all the reads are compared to a library of the host genome and matches are removed.
% phix removal ?, explanation right, tmi?
In Illumina sequencing the PhiX library can used as an internal standard. It is a library of well maintained and characterized phage genomes. They are not always removed correctly when using publicly available datasets. In order to ensure no contamination of the reads they can be removed by comparing to the known genome. 

	\subsection{Taxonomic profiling}
After preprocessing the more interesting questions can be asked. What kind of microbes are in the sample?

% taxonomic tree, insert graphics, 
% explain clades
In order to classify groups of organisms and display their hierarchy biologists use the taxonomic rank. The main taxonomic ranks from generic to specific are: domain, kingdom, phylum, class, order, family, genus and species. For metagenomics the genus and species level are used the most. 
In 2020 3,433 (Â± 115) bacterial genus names were accepted \citep{genus}.
Strains are subtypes of species which is extremely useful for a even more fine grained resolution of the sample composition.
% genus size, species size

Taxonomic profilers use preprocessed reads to estimate the relative composition of microbes in the probe.
There are different kinds of taxonomic profilers, MetaPhlan3 and kraken2 are two widely used examples. 

% make mpa and kraken subsubsections?
%\subsubsection{MetaPhlAn3}
% way to unspecific
MetaPhlAn3 (\textbf{Meta}genomic \textbf{Ph}y\textbf{l}ogenetic \textbf{An}alysis) is the third iteration of a taxonomic profiling software from the Segata Lab. It is reference based, which means it uses the coverage of clade specific marker genes in a sample to estimate the taxonomic composition.
The marker genes are chosen such that all strains in a clade posses them while other clades contain no orthologes which are close enough to match as well. \citep{mpa3}
MetaPhlAn3 contains 1.1M marker genes from around 13.5 k species.

%\subsubsection{kraken2}
% way to unspecific
kraken2 is reference based and uses a huge database to compare the sample to. From the query sample k-mers are extracted and matches in the database to assign the Lowest Common Ancestor (LCA). This information is used to estimate the relative species composition of the sample. \citep{kraken2}



\section{Machine Learning}
In order to get meaningful results with ML problems have to be framed correctly. The goal of a study determines the methods and algorithms that have to be used. 
What is the objective has to be one of the first questions asked. Is it sufficient to put samples in two or more distinct categories or are the samples differing in severity? The first would be a classification problem while the second case would be a regression problem.   
% mention clustering?
Another important distinction of ML algorithms is weather they are using supervised or unsupervised learning. This is dependent on the data as supervised algorithms need clearly labeled training data. The training data is used to optimize the performance so that unseen data can be categorized correctly by the algorithm. Unsupervised learning uses untagged data where algorithms try to find underlying patterns.

A pitfall to be aware of when using supervised learning problems is overfitting. The internal model used is to complex. This means that the algorithm has learned specifics of the training data and it is not generalizing well. It then has problems predicting new and unseen data. 

Cross-validation is a technique that can be used to asses the generalization of an given algorithm. It can thus be used to prevent overfitting. 
Data is split in training and validation set, this can be done multiple times following certain schemes.
% leave-one-out, k-fold, nested

Feature selection can also be used to prevent overfitting. It can shorten training times and help to avoid the curse of dimensionality.
% how does it work?

% Hyperparamters and tuning
Implementations of ML algorithms usually need more input than the training data, they have so called hyperparameters which need to be adjusted in order to optimize the results. Each algorithm has its own set of hyperparameters and the tuning of them is a very important but time intensive step.

In recent years a wide range of ML algorithms has been developed. The following sections describe some of the most widely used algorithms for supervised classification.

	\subsection{Support Vector Machines}
Suppose a dataset with two classes. In order to classify the data points a SVM will try to construct a hyperplane which separates the data points of those two classes. 
There can be many ways that a hyperplane separates the data so a reasonable choice is to maximize the margin, which is the distance between the data points and the hyperplane. % a bit to vague, maybe?
Nonlinear data can be classified with a SVM as well. To enable this the kernel trick is needed. The feature space is transformed to a higher dimension which makes it possible to fit a linear hyperplane. For this polynomial or Gaussian radial basis function kernels can be used.

% add formal mathmatical definitions!

% what are the hyperparameters
 
	\subsection{Random Forests}
Random forests are a ensemble learning method taking multiple decision trees and averaging their results.

%Decision tree
%observation represented in branches (features), target value in leaves (classification)

Individual decision trees tend to overfit the training data. RF learning averages over multiple deep trees trained from different parts of data set. This reduces variance, but leads to a small increase in bias and a marginal decreased interpretability compared to individual decision trees. Generally the interpretability is a wished feature and it is still given through the variable importance.

%bootstrap aggregating (bagging)
%bagging decreases variance, without increasing the bias

% what are the hyperparameters

	\subsection{Gradient Boosted Trees}
Ensemble method
boosting: multiple weak learners into a strong one
iteratively learning weak learners, adding to the strong learner after adding data weights are readjusted (re-weighing) misclassified input gets higher weight so future learners focus more on them
weak learners are added using a gradient descent optimization algorithm


% what are the hyperparameters


	\subsection{Evaluation of Machine Learning algorithms}
Different metrics have been proposed and are being used to optimize and evaluate the effectiveness of ML algorithms. Which of these are best is not clear. There are no standards yet as the importance of respective advantages and disadvantages are still being debated. It is better to use many metrics and not completely rely on one.

% roc
%(balanced) accuracy
% precision
% recall
% f1-score
% mcc




\section{Workflowmanagers and Reproducibility}
In order to generate meaningful results preprocessing, profiling, training and prediction have to be brought together. For this different scripts have to be executed sequentially. For one dataset this is fairly straight forward. But for multiple datasets it can get repetitive and error prone as the same scripts have to be executed over and over with slightly different parameters.  
Workflow management frameworks can be used to build pipelines which automate the execution of scripts and thus help to avoid time costly errors. In combination with containers they can enable experiments being conducted independent of the hardware used making them portable. 
Using a workflowmanager is a big step towards reproducibility as the execution of a pipeline with the same parameters should yield the same result. This only holds if there inherent randomness in the pipeline. For machine learning this means that only CPU powered learning can be truly reproducible and only if all seeds are set to fixed values.
% sklearn for example
% mention GPUs?

A lot of workflow management frameworks have been developed and new ones are being published continuously. Snakemake and Nextflow are among the most popular as they have strong community support.
Both are Command Line Interface (CLI) programs and support the usage of containers as well as version control systems like git. They use their own Domain Specific Language (DSL) as extensions of the underlying programming language, Groovy for Nextflow and Python for Snakemake.

Snakemake executes operations based on a directed acyclic graph (DAG). Each step is represented by rules which describe the handling of input and output files. The DAG is derived inferred from the set of all rules  \citep{snakemake}. 

In the Nextflow dataflow model operations are executed in their own isolated process. The output of a process then is streamed through a channel into processes dependent on the previous output. This enables parallel execution of tasks and a high scalability \citep{nextflow}.
Around Nextflow a vivid community has developed with nf-core. Best practices pipelines for a wide range of use cases have been developed and are being curated. They are easily accessible and have extensive documentation. Questions can be asked in the very active Slack workspace. A starter template helps developers following best practices \citep{nfcore}.

\section{Previous work}
strengths and weaknesses 
reasoning and importance of my work, placement in field of research -> no pipeline yet, stuff is not reproducible 
% maybe better in intro/objective ?



